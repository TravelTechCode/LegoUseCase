# LegoUseCase


## ‚úÖ Assumptions

1. **Focus on Spool Output, Not Job Execution Metrics**  
   The framework is **intended to capture and transmit the output data generated by ECC batch jobs**, as found in their **spool**. It does **not** handle
   job execution-level KPIs or statistics, such as:
   - Job runtime duration
   - Job start/end timestamps
   - Job execution status (e.g., success, error)
   - Job step logs or exceptions
   - Resource usage (memory, CPU)
   - Job dependency or scheduling hierarchy

   These metrics can be handled via separate monitoring tools like SAP Solution Manager, SAP Job Monitoring.

3. **Collector Responsibility**  
   Each collector handler is responsible for:
   - Spool identification and reading logic
   - Transforming the unstructured text into structured JSON
   - Handling parsing edge cases for its respective batch job

4. **ZBATCH_DATA is a Generic Store**  
   The table `ZBATCH_DATA` is designed to store JSON from **multiple collectors with varying data models**, unified only by collector name and metadata.
   The consumers of this data (like CDS/Fiori/API) must parse and interpret the JSON dynamically.

---

## üìå End-to-End Process Overview

1. **Fetch Collectors**  
   The custom framework reads a list of registered collector handlers from the table `ZCOLLECTOR_REG`.

2. **Invoke Collector Handlers**  
   Each collector represents a specific ECC batch job. The framework invokes the respective ABAP class to handle the job.

3. **Spool Data to JSON**  
   Each collector reads the spool output of its respective batch job, parses the data, and converts it to JSON format.

4. **Save to ZBATCH_DATA**  
   The framework saves the JSON payload and metadata (e.g., job name, execution time, record count, status) into the generic table `ZBATCH_DATA`.

5. **Send to AWS Cloud API**  
   The framework then invokes a separate method to push this JSON payload to a designated AWS Cloud API endpoint.

6. **Expose via Fiori**  
   A CDS View on top of `ZBATCH_DATA` is used to expose this data, which is visualized in a unified Fiori UI.

---

## üèóÔ∏è Architecture Diagram

![Batch Monitoring Architecture](./batch-monitoring-architecture.png)

---
#  Batch Collector Logging from SAP ECC to AWS

## üìò Overview

To send batch job data entries from the SAP ECC system into an external system (e.g., AWS), using an event-driven approach. 

The ECC system writes to a custom table (`zbatch_data`), and based on this, a trigger logic posts the data to AWS either directly or through SAP Event Mesh.

---

‚öôÔ∏è Solution Options

‚úÖ Option A: Direct REST Call to AWS from ECC

üîó Flow
New entry is created in `zbatch_data`.

ABAP sends a POST request directly to the AWS REST API using CL_HTTP_CLIENT.

Approach 1: Asynchronous Execution with CALL FUNCTION ... STARTING NEW TASK

    Create a Remote-enabled Function Module. within the FM using cl_http_client trigger the call to AWS.
    Next call the FM 
    <pre> CALL FUNCTION 'Z_SEND_TO_AWS_ASYNC'
    STARTING NEW TASK 'AWS_SEND'
    DESTINATION 'NONE' </pre>
    
Approach 2 :

   Create a Remote-enabled Function Module. within the FM using cl_http_client trigger the call to AWS.
   Create bgRFC Destination.
   Call the FM via CALL FUNCTION IN BACKGROUND UNIT

   <pre> CALL FUNCTION 'Z_SEND_TO_AWS_BGRFC' IN BACKGROUND UNIT lv_unit
  DESTINATION INBOUND 'Z_AWS_LOGGING'
  EXPORTING
    iv_job_id     = lv_job_id
    iv_status     = lv_status
    iv_timestamp  = lv_timestamp
    iv_message    = lv_message. </pre>

Key Benefits of bgRFC over STARTING NEW TASK is the bgRFC has inbuilt retry mechanism.


‚úÖ Option B: Use SAP Event Mesh for Messaging

üîó Flow
Data is inserted in `zbatch_data`.

Topic subscription on creation of entries in the custom table.(here the feasibility needs to be checked)

AWS is subscribed to the topic(ECC Batch job data) Event Mesh .

AWS handles the message asynchronously.

üîß Setup Requirements
SAP Event Mesh service on BTP

SAP ECC connected via SAP Cloud Connector

Queue/topic configured for messaging

Destination set up in BTP cockpit for ECC to publish

The Add-On SAP NetWeaver, add-on for event enablement (ASANWEE)

üîß ABAP to Publish Event to Event Mesh
Use CL_HTTP_CLIENT to call Event Mesh REST API .

---

üì± Fiori App Design

To visualize SAP ECC Batch Job Collector data, where entries are stored as JSON payloads in a backend table (e.g., zbatch_data). 
The app reads this data and dynamically renders a table UI with headers and values based on the keys and structure of each JSON.

This approach ensures flexibility in presenting tabular data where structure can vary across scenarios.


---

üß± Architecture Overview

Layer	       &nbsp;&nbsp;&nbsp;&nbsp; Technology

Frontend     &nbsp;&nbsp;&nbsp;&nbsp; SAPUI5 / Fiori freestyle 

Backend      &nbsp;&nbsp;&nbsp;&nbsp; SAP Gateway (ECC OData)

Integration	 &nbsp;&nbsp;&nbsp;&nbsp; OData service or BTP CAP app proxying to ECC

Data Source  &nbsp;&nbsp;&nbsp;&nbsp; ECC Table ZBATCHCOLLECTOR_DATA storing JSON payloads

---

üñ•Ô∏è Fiori App UI Structure

Fiori Elements App (List Report + Object Page)

**List Report Page** <br>

Filters: Collector, Status, Date Range

Table: Shows summary (collector, job name, record count, status)

Navigation to detail page<br>

**Object Page** <br>

Header: Collector + Job spool data

Section: Parsed JSON as dynamically rendered table based on the json structure.   

Use a manually constructed sap.m.Table where:

Dynamically generate table columns and rows based on the JSON keys and values.

This approach gives full flexibility when the JSON payload has a variable structure.

üîß Steps:

Parse the JSON payload from the backend.

Dynamically extract keys and generate columns.

Bind the payload to a JSONModel.

```` const jsonData = <your payload>; // e.g., from backend
const keys = Object.keys(jsonData[0]);

keys.forEach(key => {
  oTable.addColumn(new sap.m.Column({
    header: new sap.m.Label({ text: key })
  }));
});

oTable.bindItems({
  path: "/",
  template: new sap.m.ColumnListItem({
    cells: keys.map(key => new sap.m.Text({ text: `{${key}}` }))
  })
});
````

---

## ‚ö†Ô∏è Error Handling and Failure Points

| Stage                         | Potential Failure                             | Handling Strategy                                                   |
|------------------------------|-----------------------------------------------|---------------------------------------------------------------------|
| Fetching collectors          | Table empty, invalid/inactive collectors      | Skip invalid entries, log missing handlers                         |
| Calling handlers             | Missing class, unhandled exception            | Wrap in TRY-CATCH, update `status` and `message` in `ZBATCH_DATA`  |
| Spool reading & parsing      | Spool not found, empty/corrupt data           | Validate spool, handle edge cases, log errors                      |
| JSON conversion              | Malformed structure, large payload            | Catch conversion errors, truncate or split large JSON              |
| Saving to ZBATCH_DATA        | DB insert failure, key collision              | Use `INSERT OR UPDATE`, log SQL exceptions                         |
| Cloud API call               | Network issues, auth failure, 4xx/5xx response| Handle HTTP response, retry with backoff, update `status` field    |
| Fiori JSON visualization     | JSON structure varies, parse error on UI      | Validate JSON before saving, implement fallback UI behavior        |

---

## üîß Additional Recommendations

- Central logging using a utility class (e.g., `ZLOGGER`)
- Background job retry mechanism for failed collectors
- Notification via email or monitoring tools in case of repeated failures
- Unit testing for collector classes and JSON payloads

